{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2fe2c26",
   "metadata": {},
   "source": [
    "# Advanced SMS Spam Detection with DistilBERT\n",
    "\n",
    "This notebook implements a comprehensive SMS spam detection system using advanced preprocessing techniques and DistilBERT fine-tuning. The implementation follows best practices for mobile deployment and includes hyperparameter optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8513217",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, we'll install and import all required libraries for our SMS spam detection pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf77a7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.57.0)\n",
      "Requirement already satisfied: torch in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: optuna in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: spacy in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: emoji in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: regex in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2025.9.18)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.7)\n",
      "Requirement already satisfied: seaborn in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (1.17.0)\n",
      "Requirement already satisfied: colorlog in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from optuna) (2.0.43)\n",
      "Requirement already satisfied: click in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (0.19.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (2.11.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.2.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: Mako in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bdcalling123\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch pandas numpy scikit-learn optuna nltk spacy emoji regex tqdm matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f14b26bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bdcalling123\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bdcalling123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bdcalling123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\bdcalling123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\bdcalling123\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW  # Import AdamW from torch.optim instead of transformers\n",
    "from transformers import (\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertModel,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    roc_auc_score, confusion_matrix\n",
    ")\n",
    "import optuna\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import emoji\n",
    "import logging\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624f9df4",
   "metadata": {},
   "source": [
    "## 2. Load and Explore SMS Data\n",
    "\n",
    "We'll load the SMS dataset and perform initial exploration to understand its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df17adf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (5572, 5)\n",
      "\n",
      "First few rows:\n",
      "     v1                                                 v2 Unnamed: 2  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "5  spam  FreeMsg Hey there darling it's been 3 week's n...        NaN   \n",
      "6   ham  Even my brother is not like to speak with me. ...        NaN   \n",
      "7   ham  As per your request 'Melle Melle (Oru Minnamin...        NaN   \n",
      "8  spam  WINNER!! As a valued network customer you have...        NaN   \n",
      "9  spam  Had your mobile 11 months or more? U R entitle...        NaN   \n",
      "\n",
      "  Unnamed: 3 Unnamed: 4  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n",
      "5        NaN        NaN  \n",
      "6        NaN        NaN  \n",
      "7        NaN        NaN  \n",
      "8        NaN        NaN  \n",
      "9        NaN        NaN  \n",
      "\n",
      "Class distribution:\n",
      "v1\n",
      "ham     0.865937\n",
      "spam    0.134063\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Message length statistics:\n",
      "       count        mean        std   min    25%    50%    75%    max\n",
      "v1                                                                   \n",
      "ham   4825.0   71.023627  58.016023   2.0   33.0   52.0   92.0  910.0\n",
      "spam   747.0  138.866131  29.183082  13.0  132.5  149.0  157.0  224.0\n"
     ]
    }
   ],
   "source": [
    "# Load the SMS dataset\n",
    "df = pd.read_csv(\"Datasets/spam (1).csv\", encoding='latin1')\n",
    "\n",
    "# Basic data exploration\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head(10))\n",
    "\n",
    "# Class distribution\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['v1'].value_counts(normalize=True))\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nMessage length statistics:\")\n",
    "df['length'] = df['v2'].str.len()\n",
    "print(df.groupby('v1')['length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a9381e",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing Pipeline\n",
    "\n",
    "Implementing a comprehensive preprocessing pipeline specifically designed for SMS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b5b80c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vs Processed Text Examples:\n",
      "\n",
      "Original: Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "Processed: go jurong point , crazy .. available bugis n great world la e buffet .. cine got amore wat ..\n",
      "\n",
      "Original: Ok lar... Joking wif u oni...\n",
      "Processed: ok lar .. joking wif oni ..\n",
      "\n",
      "Original: Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
      "Processed: free entry wkly comp win fa cup final tkts 21st may 2005. text fa 87121 receive entry question ( std text rate ) & c 's apply 0 < phone > over18 's\n"
     ]
    }
   ],
   "source": [
    "class SMSPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Common SMS abbreviations\n",
    "        self.sms_dict = {\n",
    "            \"u\": \"you\", \"ur\": \"your\", \"2\": \"to\", \"4\": \"for\",\n",
    "            \"b4\": \"before\", \"gr8\": \"great\", \"luv\": \"love\",\n",
    "            \"msg\": \"message\", \"txt\": \"text\", \"asap\": \"as soon as possible\"\n",
    "        }\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"\n",
    "        Complete preprocessing pipeline for SMS messages\n",
    "        \"\"\"\n",
    "        # Stage 1: Initial Text Cleaning\n",
    "        text = self._initial_cleaning(text)\n",
    "        \n",
    "        # Stage 2: SMS-Specific Preprocessing\n",
    "        text = self._sms_specific_preprocessing(text)\n",
    "        \n",
    "        # Stage 3: Advanced Linguistic Processing\n",
    "        text = self._linguistic_processing(text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _initial_cleaning(self, text):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Replace URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '<url>', text, flags=re.MULTILINE)\n",
    "        \n",
    "        # Replace email addresses\n",
    "        text = re.sub(r'\\S+@\\S+', '<email>', text)\n",
    "        \n",
    "        # Replace phone numbers\n",
    "        text = re.sub(r'\\+?[1-9][0-9\\-\\(\\)]{8,}', '<phone>', text)\n",
    "        \n",
    "        # Handle emoji\n",
    "        text = emoji.replace_emoji(text, '<emoji>')\n",
    "        \n",
    "        # Normalize whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _sms_specific_preprocessing(self, text):\n",
    "        # Expand abbreviations\n",
    "        words = text.split()\n",
    "        words = [self.sms_dict.get(word, word) for word in words]\n",
    "        text = ' '.join(words)\n",
    "        \n",
    "        # Handle repeated characters (e.g., \"sooooo\" → \"so\")\n",
    "        text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
    "        \n",
    "        # Normalize numbers\n",
    "        text = re.sub(r'\\$\\d+(\\.\\d{2})?', '<money>', text)\n",
    "        text = re.sub(r'\\d+%', '<percentage>', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _linguistic_processing(self, text):\n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Selective stop word removal (keep potentially important ones)\n",
    "        important_stops = {'not', 'no', 'never', 'none', 'free', 'click'}\n",
    "        tokens = [token for token in tokens if token not in self.stop_words or token in important_stops]\n",
    "        \n",
    "        # Light lemmatization (only for verbs and nouns)\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = SMSPreprocessor()\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "df['processed_text'] = df['v2'].apply(preprocessor.preprocess)\n",
    "\n",
    "# Show some examples\n",
    "print(\"Original vs Processed Text Examples:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nOriginal: {df['v2'].iloc[i]}\")\n",
    "    print(f\"Processed: {df['processed_text'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569a74ed",
   "metadata": {},
   "source": [
    "## 4. Build DistilBERT Model\n",
    "\n",
    "Now we'll create our custom model architecture using DistilBERT as the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c30c8a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture:\n",
      "SMSSpamClassifier(\n",
      "  (distilbert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          (attention): DistilBertSdpaAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation()\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SMSSpamClassifier(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.3):\n",
    "        super(SMSSpamClassifier, self).__init__()\n",
    "        \n",
    "        # Load pre-trained DistilBERT\n",
    "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        \n",
    "        # Freeze some layers for efficiency (optional)\n",
    "        for param in self.distilbert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Unfreeze the last 2 transformer layers\n",
    "        for layer in self.distilbert.transformer.layer[-2:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "        # Classification head\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier = nn.Linear(self.distilbert.config.hidden_size, 2)\n",
    "        \n",
    "        # Ensure model parameters are float32\n",
    "        self.to(torch.float32)\n",
    "        \n",
    "        # Attention visualization layer\n",
    "        self.attention_weights = None\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Get DistilBERT outputs\n",
    "        outputs = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=True\n",
    "        )\n",
    "        \n",
    "        # Get the [CLS] token representation\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # Store attention weights for visualization\n",
    "        self.attention_weights = outputs.attentions[-1]  # Last layer's attention\n",
    "        \n",
    "        # Apply dropout and classification\n",
    "        x = self.dropout(cls_output)\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def get_attention_weights(self):\n",
    "        return self.attention_weights\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = SMSSpamClassifier().to(device)\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c66efa",
   "metadata": {},
   "source": [
    "## 5. Training Configuration\n",
    "\n",
    "Set up the training parameters, loss function, optimizer, and learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "398da90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "Number of training examples: 3900\n",
      "Number of validation examples: 836\n",
      "Number of test examples: 836\n",
      "Number of training steps: 732\n",
      "Number of warmup steps: 73\n",
      "Class weights: [1.       6.456979]\n"
     ]
    }
   ],
   "source": [
    "# Create PyTorch Dataset\n",
    "class SMSDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Tokenize text\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Ensure float32 type for input tensors\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten().to(torch.long),  # Use long for input_ids\n",
    "            'attention_mask': encoding['attention_mask'].flatten().to(torch.float32),  # Use float32 for attention_mask\n",
    "            'label': torch.tensor(label, dtype=torch.long)  # Use long for labels\n",
    "        }\n",
    "\n",
    "# Prepare data splits\n",
    "X = df['processed_text'].values\n",
    "y = (df['v1'] == 'spam').astype(int).values\n",
    "\n",
    "# Create stratified train, validation, and test splits\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, stratify=y, random_state=42\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.15/0.85, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SMSDataset(X_train, y_train, tokenizer)\n",
    "val_dataset = SMSDataset(X_val, y_val, tokenizer)\n",
    "test_dataset = SMSDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Training configuration\n",
    "config = {\n",
    "    'learning_rate': 2e-5,\n",
    "    'num_epochs': 3,\n",
    "    'warmup_steps': 0.1,  # 10% of total steps\n",
    "    'weight_decay': 0.01,\n",
    "    'gradient_clipping': 1.0\n",
    "}\n",
    "\n",
    "# Calculate total steps for warmup\n",
    "total_steps = len(train_loader) * config['num_epochs']\n",
    "warmup_steps = int(total_steps * config['warmup_steps'])\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# Initialize loss function with class weights to handle imbalance\n",
    "# Ensure class weights are float32\n",
    "class_weights = torch.tensor(\n",
    "    [1.0, (y_train == 0).sum() / (y_train == 1).sum()],\n",
    "    device=device,\n",
    "    dtype=torch.float32  # Explicitly set dtype to float32\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"Number of training examples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation examples: {len(val_dataset)}\")\n",
    "print(f\"Number of test examples: {len(test_dataset)}\")\n",
    "print(f\"Number of training steps: {total_steps}\")\n",
    "print(f\"Number of warmup steps: {warmup_steps}\")\n",
    "print(f\"Class weights: {class_weights.cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6ac5ad",
   "metadata": {},
   "source": [
    "## 6. Model Training and Validation\n",
    "\n",
    "Implement the training loop with early stopping and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a7b4fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/244 [00:00<?, ?it/s]DistilBertSdpaAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "DistilBertSdpaAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "Training: 100%|██████████| 244/244 [05:38<00:00,  1.39s/it, loss=0.0170]\n",
      "Training: 100%|██████████| 244/244 [05:38<00:00,  1.39s/it, loss=0.0170]\n",
      "Evaluating: 100%|██████████| 27/27 [00:34<00:00,  1.27s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.2406 | Train Acc: 0.9121\n",
      "Val Loss: 0.0457 | Val Acc: 0.9880\n",
      "Val Precision: 0.9397 | Val Recall: 0.9732\n",
      "Val F1: 0.9561\n",
      "\n",
      "Epoch 2/3\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 244/244 [05:48<00:00,  1.43s/it, loss=0.0028]\n",
      "Training: 100%|██████████| 244/244 [05:48<00:00,  1.43s/it, loss=0.0028]\n",
      "Evaluating: 100%|██████████| 27/27 [00:36<00:00,  1.36s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1204 | Train Acc: 0.9882\n",
      "Val Loss: 0.0455 | Val Acc: 0.9928\n",
      "Val Precision: 0.9649 | Val Recall: 0.9821\n",
      "Val F1: 0.9735\n",
      "\n",
      "Epoch 3/3\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 244/244 [05:33<00:00,  1.37s/it, loss=0.0020]\n",
      "Training: 100%|██████████| 244/244 [05:33<00:00,  1.37s/it, loss=0.0020]\n",
      "Evaluating: 100%|██████████| 27/27 [00:34<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0835 | Train Acc: 0.9918\n",
      "Val Loss: 0.0456 | Val Acc: 0.9928\n",
      "Val Precision: 0.9649 | Val Recall: 0.9821\n",
      "Val F1: 0.9735\n",
      "\n",
      "Training completed!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, scheduler, clip_value=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc='Training')\n",
    "    for batch in progress_bar:\n",
    "        # Get batch data\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Update metrics\n",
    "        total_loss += loss.item()\n",
    "        predictions.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def evaluate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc='Evaluating'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            predictions.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, predictions, average='binary'\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'predictions': predictions,\n",
    "        'true_labels': true_labels\n",
    "    }\n",
    "\n",
    "# Training loop with early stopping\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_patience = 3\n",
    "early_stopping_counter = 0\n",
    "training_history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_acc': [],\n",
    "    'val_f1': []\n",
    "}\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{config['num_epochs']}\")\n",
    "    \n",
    "    # Training phase\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, criterion, optimizer,\n",
    "        scheduler, config['gradient_clipping']\n",
    "    )\n",
    "    \n",
    "    # Validation phase\n",
    "    val_metrics = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    # Update training history\n",
    "    training_history['train_loss'].append(train_loss)\n",
    "    training_history['train_acc'].append(train_acc)\n",
    "    training_history['val_loss'].append(val_metrics['loss'])\n",
    "    training_history['val_acc'].append(val_metrics['accuracy'])\n",
    "    training_history['val_f1'].append(val_metrics['f1'])\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_metrics['loss']:.4f} | Val Acc: {val_metrics['accuracy']:.4f}\")\n",
    "    print(f\"Val Precision: {val_metrics['precision']:.4f} | Val Recall: {val_metrics['recall']:.4f}\")\n",
    "    print(f\"Val F1: {val_metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_metrics['loss'] < best_val_loss:\n",
    "        best_val_loss = val_metrics['loss']\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7e3b52",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Optimization\n",
    "\n",
    "Use Optuna for systematic hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e4c28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-15 01:21:00,957] A new study created in memory with name: no-name-285c3961-b602-4af3-bba9-5fba5d6d8b6d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running hyperparameter optimization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 122/122 [05:20<00:00,  2.63s/it, loss=0.0624]\n",
      "Training: 100%|██████████| 122/122 [05:20<00:00,  2.63s/it, loss=0.0624]\n",
      "Evaluating: 100%|██████████| 27/27 [00:29<00:00,  1.08s/it]\n",
      "Evaluating: 100%|██████████| 27/27 [00:29<00:00,  1.08s/it]\n",
      "Training: 100%|██████████| 122/122 [04:40<00:00,  2.30s/it, loss=0.0129]\n",
      "Training: 100%|██████████| 122/122 [04:40<00:00,  2.30s/it, loss=0.0129]\n",
      "Evaluating: 100%|██████████| 27/27 [00:33<00:00,  1.23s/it]\n",
      "Evaluating: 100%|██████████| 27/27 [00:33<00:00,  1.23s/it]\n",
      "Training: 100%|██████████| 122/122 [05:19<00:00,  2.61s/it, loss=0.0063]\n",
      "Training: 100%|██████████| 122/122 [05:19<00:00,  2.61s/it, loss=0.0063]\n",
      "Evaluating: 100%|██████████| 27/27 [00:33<00:00,  1.25s/it]\n",
      "[I 2025-10-15 01:37:59,018] Trial 0 finished with value: 0.9691629955947136 and parameters: {'learning_rate': 2.0471231850010034e-05, 'batch_size': 32, 'dropout_rate': 0.3756686765825775, 'weight_decay': 0.010838916812871116, 'warmup_steps': 0.11386301952033767}. Best is trial 0 with value: 0.9691629955947136.\n",
      "Evaluating: 100%|██████████| 27/27 [00:33<00:00,  1.25s/it]\n",
      "[I 2025-10-15 01:37:59,018] Trial 0 finished with value: 0.9691629955947136 and parameters: {'learning_rate': 2.0471231850010034e-05, 'batch_size': 32, 'dropout_rate': 0.3756686765825775, 'weight_decay': 0.010838916812871116, 'warmup_steps': 0.11386301952033767}. Best is trial 0 with value: 0.9691629955947136.\n",
      "Training: 100%|██████████| 244/244 [05:18<00:00,  1.31s/it, loss=0.0009]\n",
      "Training: 100%|██████████| 244/244 [05:18<00:00,  1.31s/it, loss=0.0009]\n",
      "Evaluating: 100%|██████████| 53/53 [00:34<00:00,  1.54it/s]\n",
      "Evaluating: 100%|██████████| 53/53 [00:34<00:00,  1.54it/s]\n",
      "Training:  78%|███████▊  | 190/244 [04:13<01:17,  1.43s/it, loss=0.0026]"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    # Hyperparameters to optimize\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 5e-5, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [8, 16, 32]),\n",
    "        'dropout_rate': trial.suggest_float('dropout_rate', 0.1, 0.5),\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 0.01, 0.1),\n",
    "        'warmup_steps': trial.suggest_float('warmup_steps', 0.1, 0.3)\n",
    "    }\n",
    "    \n",
    "    # Create model with trial parameters\n",
    "    model = SMSSpamClassifier(dropout_rate=params['dropout_rate']).to(device)\n",
    "    \n",
    "    # Create data loaders with trial batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
    "    \n",
    "    # Training setup\n",
    "    total_steps = len(train_loader) * config['num_epochs']\n",
    "    warmup_steps = int(total_steps * params['warmup_steps'])\n",
    "    \n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=params['learning_rate'],\n",
    "        weight_decay=params['weight_decay']\n",
    "    )\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        # Training phase\n",
    "        train_loss, _ = train_epoch(\n",
    "            model, train_loader, criterion, optimizer,\n",
    "            scheduler, config['gradient_clipping']\n",
    "        )\n",
    "        \n",
    "        # Validation phase\n",
    "        val_metrics = evaluate(model, val_loader, criterion)\n",
    "        \n",
    "        # Report intermediate value\n",
    "        trial.report(val_metrics['f1'], epoch)\n",
    "        \n",
    "        # Handle pruning based on the intermediate value\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        best_val_f1 = max(best_val_f1, val_metrics['f1'])\n",
    "    \n",
    "    return best_val_f1\n",
    "\n",
    "# Create study object and specify the direction is 'maximize' the F1 score\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Run hyperparameter optimization\n",
    "print(\"Running hyperparameter optimization...\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Plot optimization history\n",
    "optuna.visualization.plot_optimization_history(study)\n",
    "optuna.visualization.plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba13d570",
   "metadata": {},
   "source": [
    "## 8. Evaluation and Visualization\n",
    "\n",
    "Calculate metrics and visualize model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "# Evaluate on test set\n",
    "test_metrics = evaluate(model, test_loader, criterion)\n",
    "\n",
    "# Print final metrics\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "print(f\"F1 Score: {test_metrics['f1']:.4f}\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(test_metrics['true_labels'], test_metrics['predictions'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot losses\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(training_history['train_loss'], label='Training Loss')\n",
    "plt.plot(training_history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plot metrics\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(training_history['train_acc'], label='Train Accuracy')\n",
    "plt.plot(training_history['val_acc'], label='Val Accuracy')\n",
    "plt.plot(training_history['val_f1'], label='Val F1')\n",
    "plt.title('Training Metrics')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Attention visualization for a sample message\n",
    "def visualize_attention(model, tokenizer, text):\n",
    "    # Tokenize and prepare input\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        return_tensors='pt',\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Get model prediction and attention weights\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        attention = model.get_attention_weights()\n",
    "    \n",
    "    # Get prediction\n",
    "    pred = outputs.argmax(dim=1).item()\n",
    "    \n",
    "    # Get tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    \n",
    "    # Plot attention heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        attention[0][0].cpu(),\n",
    "        xticklabels=tokens,\n",
    "        yticklabels=tokens,\n",
    "        cmap='viridis'\n",
    "    )\n",
    "    plt.title(f'Attention Visualization (Predicted: {\"spam\" if pred == 1 else \"ham\"})')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize attention for a sample spam message\n",
    "sample_text = X_test[y_test == 1][0]  # Get a spam message\n",
    "print(\"Sample text:\", sample_text)\n",
    "visualize_attention(model, tokenizer, sample_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
